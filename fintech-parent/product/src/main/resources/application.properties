spring.application.name=product
spring.jpa.hibernate.ddl-auto=create-drop
spring.datasource.url=jdbc:mysql://product-db:3309/product?allowPublicKeyRetrieval=true&useSSL=false&useTimezone=true&serverTimezone=UTC
spring.datasource.username=product_user
spring.datasource.password=product_password
spring.jpa.show-sql: true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
##observabiltity 

management.endpoints.web.exposure.include=health,env,metrics,beans,loggers
management.endpoint.health.enabled=true
management.endpoint.metrics.enabled=true
management.endpoint.beans.enabled=true
management.endpoint.loggers.enabled=true
management.endpoint.env.enabled=true

# All traces should be sent to latency analysis tool
management.tracing.sampling.probability=1.0
#management.zipkin.tracing.endpoint: http://zipkin:9411/api/v2/spans
#management.endpoints.web.exposure.include=prometheus
#tracing.url= http://jaeger:4318/v1/traces

management.otlp.tracing.endpoint=http://jaeger:4318/v1/traces
# traceID and spanId are predefined MDC keys - we want the logs to include them
logging.pattern.level=%5p [${spring.application.name:},%X{traceId:-},%X{spanId:-}]
#logging.level.root=debug
spring.cloud.stream.kafka.binder.enable-observation=true


spring.kafka.bootstrap-servers=my-cluster-kafka-bootstrap:9091,my-cluster-kafka-bootstrap:9092,my-cluster-kafka-bootstrap:9093

#spring.cloud.stream.kafka.binder.brokers=my-cluster-kafka-brokers
#spring.cloud.stream.kafka.binder.auto-add-partitions=false
#spring.cloud.stream.kafka.binder.consider-down-when-any-partition-has-no-leader=false
##spring.cloud.stream.kafka.binder.brokers=my-cluster-kafka-brokers:9092
#spring.cloud.stream.kafka.binder.brokers=9092
##spring.cloud.stream.kafka.binder.zkNodes=my-cluster-zookeeper-nodes
#spring.cloud.stream.kafka.binder.defaultZkPort=2181
#spring.kafka.brokers=zookeeper:2181
#spring.kafka.admin.auto-create=false
# spring.cloud.stream.default.group=product
spring.cloud.function.definition=handleOrderEvent 
#handleOrderCreation;handleOrderDeletion
spring.cloud.stream.bindings.handleOrderEvent-in-0.destination: order-flow
#spring.cloud.stream.bindings.handleOrderCreation-in-0.destination: order-flow
#spring.cloud.stream.bindings.handleOrderDeletion-in-0.destination: order-flow

#spring.cloud.stream.bindings.handleOrderCreation-in-0.consumer.
#spring.cloud.stream.kafka.binder.auto-create-topics=false
spring.cloud.stream.bindings.handleOrderEvent-in-0.group=ProductSrv
#spring.cloud.stream.bindings.handleOrderCreation-in-0.group=orderCreationHandler
#spring.cloud.stream.bindings.handleOrderDeletion-in-0.group=orderDeletionHandler
spring.cloud.stream.bindings.handleOrderEvent-in-0.content-type=application/json
spring.cloud.stream.kafka.bindings.handleOrderEvent-in-0.consumer.enable-dlq=true
#spring.cloud.stream.kafka.bindings.placeorderinput.consumer.enable-dlq=true
spring.cloud.stream.kafka.bindings.handleOrderEvent-in-0.consumer.dlq-name=orderErrors
spring.cloud.stream.kafka.streams.binder.deserializationExceptionHandler: sendToDlq

#spring.cloud.stream.kafka.bindings.handleOrderEvent-in-0.consumer.dlq-producer-properties
#.configuration..value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
#spring.cloud.stream.bindings.handleOrderEvent-in-0.error-handler-definition: myErrorHandler
#spring.cloud.stream.bindings.handleOrderEvent-in-0.consumer.max-attempts=3
#spring.cloud.stream.bindings.handleOrderDeletion-in-0.content-type=application/json
#spring.cloud.stream.bindings.handleOrderCreation-in-0.content-type=application/json
#//serialization configuration 
#spring.kafka.consumer.properties.spring.json.type.mapping=OrderCreatedEvent:com.fintech.events.order.OrderCreatedEvent,OrderDeletedEvent:com.fintech.events.order.OrderDeletedEvent
#spring.cloud.stream.bindings.handleOrderCreation-in-0.consumer.value-deserializer= org.springframework.kafka.support.serializer.JsonDeserializer
#spring.kafka.consumer.key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer= org.springframework.kafka.support.serializer.JsonDeserializer
spring.kafka.producer.value-serializer= org.springframework.kafka.support.serializer.JsonSerializer
#spring.kafka.producer.key-serializer: org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
spring.kafka.consumer.properties.spring.json.trusted.packages=*
spring.kafka.producer.properties.spring.json.trusted.packages=*


spring.cloud.stream.bindings.product-flow-binding.destination=product-flow
spring.cloud.stream.bindings.product-flow-binding.content-type: application/json
spring.cloud.stream.bindings.product-flow-binding.producer.use-native-encoding=true


#//routing serialization 
#spring.cloud.function.definition= functionRouter
#spring.cloud.stream.function.routing.enabled=true
#spring.cloud.function.routing-expression=headers['type'] == 'OrderCreatedEvent' ? 'handleOrderCreation' : 'handleOrderDeletion'
#spring.cloud.stream.kafka.binder.headers=type
#spring.cloud.stream.bindings.functionRouter-in-0.destination: order-flow

springwolf.docket.base-package=com.beshara.fintech.product
springwolf.enabled=true

springwolf.docket.info.title=${spring.application.name}
springwolf.docket.info.version=1.0.0
springwolf.docket.servers.kafka-server.protocol=kafka
springwolf.docket.servers.kafka-server.host=my-cluster-kafka-bootstrap:9091,my-cluster-kafka-bootstrap:9092,my-cluster-kafka-bootstrap:9093
springwolf.plugin.kafka.publishing.enabled=true
springwolf.plugin.kafka.publishing.producer.bootstrap-servers=my-cluster-kafka-bootstrap:9091,my-cluster-kafka-bootstrap:9092,my-cluster-kafka-bootstrap:9093
springwolf.plugin.kafka.publishing.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
springwolf.plugin.kafka.publishing.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer

# For debugging purposes
logging.level.io.github.springwolf=DEBUG

# Reduce/Disable kafka logging
logging.level.org.apache.kafka=ERROR
logging.level.kafka=ERROR
logging.level.state.change.logger=ERROR